# Technical Architecture

Cloud-Based Deployment

The system is deployed in the cloud to ensure scalability and ease of access. Each user session runs on a managed Ubuntu virtual machine (VM) (or containerized environment) that provides a full desktop-like environment. Users interact through a web interface that streams the desktop or terminal output, while the AI agent runs inside the VM. A centralized orchestration server manages these VMs – provisioning new instances for new sessions, shutting them down when tasks complete, and monitoring resource usage. This cloud approach means users don’t need to install anything locally, and we can easily allocate more CPU/GPU resources as needed for heavy tasks. It also facilitates rapid updates (we can deploy new agent versions to the cloud seamlessly) and ensures consistency (everyone gets the same standardized environment pre-loaded with necessary tools). For high availability, the architecture can use Kubernetes or a cloud VM scale set to spawn multiple agent instances behind a load balancer, allowing the service to accommodate many users in parallel.

Security: Each VM is isolated per user session to sandbox execution. Users access only their agent’s VM, and fine-grained cloud permissions prevent one VM from affecting others. All data in transit (between the user’s browser, the orchestrator, and the VM) is encrypted via HTTPS/WSS. Sensitive credentials (like API keys for OpenAI/Anthropic) are stored securely on the server side and injected into the VM environment as needed, never exposed to the client directly. Cloud deployment also allows us to place VMs in specific regions for compliance (e.g. EU users’ sessions in EU data centers to address GDPR concerns).

AI Model Integration

At the core of the agent is a Large Language Model (LLM) which powers its reasoning and planning capabilities. We will integrate with readily available AI models via their APIs – primarily OpenAI’s GPT-4 (or GPT-3.5) and Anthropic’s Claude, as they are state-of-the-art for understanding instructions and generating coherent plans. The agent system is designed to be model-agnostic, so we can swap in models or use multiple: for instance, use GPT-4 for complex reasoning, but possibly a cheaper/faster model for simpler tasks or intermediate steps to optimize costs. The integration is done through a Model Proxy Service in our architecture that handles API calls to AI providers. This service will format prompts with the current context (user request, current step, results from previous steps, etc.), send it to the model, and receive the response.

We leverage the models’ strengths: GPT-4 is known for excelling at complex, multi-step tasks and reasoning ￼, which aligns with our needs for planning and troubleshooting. Anthropic Claude is touted for its helpfulness and larger context window, which can be useful for keeping more extensive history of the agent’s plan in memory. Initially, the agent will rely on a single primary model to simplify development. Over time, we might incorporate a model selection layer that chooses the best model per task (for example, if a user’s task is coding-heavy, use a code-specialized model; if it’s conversational or requires a lot of world knowledge, use a more general model).

All model communications happen server-side, keeping API keys secure and allowing us to log interactions for monitoring and improvement. We will implement safeguards like usage rate limiting and cost tracking in the Model Proxy Service to control expenses from API calls (important for the pay-per-use model and to prevent abuse).

Planning and Execution Engine

The “brain” of the agent is a Planning and Execution Engine that orchestrates the step-by-step task completion. This engine works in iterative cycles:
	1.	Task Planning: When a user gives a high-level goal or instruction, the agent (via the LLM) breaks it down into a sequence of smaller actionable steps. For example, if the goal is “Find the top 5 competitors of Company X and create a comparison spreadsheet,” the agent might plan steps: (a) search the web for “Company X competitors,” (b) gather names and info, (c) open LibreOffice in the Ubuntu VM to create a sheet, (d) populate the sheet with gathered info. The plan is generated in natural language (or a simple DSL for internal use) and kept as a list of tasks.
	2.	Step Execution (Terminal or Browser): The engine then executes these steps one by one. By default, it will attempt to use a terminal command to accomplish a step if possible (since the Ubuntu VM can run commands and scripts). We have a library of known command recipes and tools. For instance, for web search, the agent might invoke a CLI like curl or a specialized script. If the step explicitly requires a web interface (e.g. interacting with a dynamic website, logging into a web account, or parsing a complex webpage), the engine can launch a browser automation process. We integrate the open-source browser-use toolkit so the agent can control a headless Chrome/Firefox via Playwright. This allows the agent to navigate websites, click links or fill forms when needed — essentially giving it “eyes and hands” on the web beyond API calls. ￼
	3.	Result Interpretation: After executing a step, the agent captures any output or outcome. For a terminal command, this could be the command’s stdout/stderr or resultant file changes. For a browser action, it could be the fetched webpage content or a screenshot/DOM extract after interaction. The LLM then interprets this result in the context of the goal. If the result provides new information, the agent updates its knowledge base (a short-term memory of facts gathered so far). If an action failed (e.g. a command error or a website not accessible), the agent notes this and considers an alternative approach.
	4.	Dynamic Re-Planning: Crucially, after each step, the Planning Engine may revise the remaining plan. New information might make some steps unnecessary or require additional steps. For example, if step 1 (web search) reveals that Company X has exactly 3 main competitors, the plan might adjust to only list 3 in the spreadsheet instead of 5, or add a step to verify a piece of data that looked inconsistent. The LLM is prompted with the updated context (“here’s what happened, should we modify the plan?”) and can insert, remove, or reorder future steps as needed. This dynamic planning ensures the agent isn’t stuck on a rigid script and can handle surprises or partial goal changes on the fly.
	5.	Loop Continuation: The cycle repeats for the next step. The engine keeps track of the overall goal, completed steps, and remaining steps. It continues until the goal is reached or no further steps are possible (at which point it will report back that it cannot complete the task or needs clarification).

This Planning and Execution Engine is essentially an implementation of a classic AI loop (Sense-Think-Act): it senses the environment through the terminal/browser, thinks by planning with the LLM, and acts by executing commands. The modular design allows swapping out components; for example, one could plugin a different planning algorithm or add specialized tool modules, but the core loop remains the same.

Self-Verification and Output Refinement

To improve reliability, the agent employs a verification and refinement mechanism after executing steps or before finalizing output. After each significant action (or set of actions), the agent uses the LLM to analyze whether the outcome makes sense and aligns with the goal. Essentially, the agent asks itself questions like: “Did the last action succeed? Have we moved closer to the user’s objective? Is the information gathered correct or does it need double-checking?”

This is implemented by a secondary prompt to the LLM where the agent’s internal transcript and results are given, and the LLM is instructed to play a “critic” role. It might say, for example, “I expected to find 5 competitors, but only 3 were found – should I search more?” or “The spreadsheet is created but maybe I should format it or verify the data accuracy.” Based on this self-review, the agent can take corrective steps: re-run a web search with different keywords, cross-verify a piece of data from another source, or adjust the output format if it’s not clear.

If the agent detects a mistake or a suboptimal result, it enters a refinement sub-loop: it formulates a fix (which could be added as new plan steps) and executes them. Only when the verification step yields a “looks good” assessment does the agent consider the task complete. This dramatically improves the final quality of results and user trust, as the agent catches many of its own errors. Industry experts note that an LLM can indeed be used to verify its own outputs or those of another model ￼, and we leverage that idea. In some cases, we could even use a second model as a dedicated checker (for instance, use GPT-4 as the main planner but a model like Claude or a GPT-4 instance with a different prompt as the reviewer for diversity of thought).

Before presenting results to the user, the agent might do a final review: e.g. if it wrote a report or created code, it will quickly scan for completeness and correctness. The result is then returned to the user along with a log of actions. Over time, this verification loop can be further enhanced with learned heuristics (like common failure modes to check for) and possibly an external knowledge base to fact-check critical information.

Integration with Ubuntu Desktop & Browser

The agent’s execution environment is an Ubuntu 22.04 LTS virtual machine image customized for our service. It includes common tools and libraries pre-installed (for example: Python for scripting, Node.js if needed, office applications for editing documents, command-line tools like curl, wget, grep, etc., and the headless Chrome/Firefox for web automation). We chose Ubuntu for its popularity and rich ecosystem of CLI tools that the agent can leverage out-of-the-box. The “desktop” can be headless (no actual GUI rendered to the user by default, to save resources), but we have the capability to stream a GUI if needed for user monitoring or for the agent to handle GUI-only applications.

Terminal Use: The agent interacts with the Ubuntu system primarily through a pseudo-terminal interface. The orchestrator sets up a secure shell or direct console connection that the agent can send commands to. Each command executed by the agent is logged and its output captured. We implement limits and timeouts to prevent runaway processes (for example, if a command hangs or takes too long, the agent will be notified and can decide to terminate it). The Ubuntu environment is ephemeral – any changes (files created, packages installed) can be discarded or snapshotted after the session, which aids security and resets state between different user tasks.

Browser Automation: When a task requires web interaction beyond what command-line tools can handle, the agent uses a browser automation driver. Using the browser-use framework (built on Playwright), the agent can launch a headless browser instance inside the VM and navigate to URLs, click buttons, fill forms, and even execute JavaScript on pages. The advantage of this approach is that even complex client-side web apps (that might not have APIs) can be accessed by the agent, essentially simulating a human user. For example, if the user asks the agent to “log in to my Facebook and post a status,” the agent could open the browser, go to facebook.com, and use stored credentials to log in and post – tasks that purely API-based bots couldn’t do on a protected site. This browser capability is used judiciously (since it’s heavier than CLI and raises more security considerations). We follow the guidance from the browser-use project which simplifies connecting AI agents to browsers ￼, ensuring our integration is robust and up-to-date with browser changes.

All interactions (terminal or browser) are funnelled back to the Planning Engine as text. For the browser, this means we often extract the page text content (stripping HTML) to feed into the LLM for analysis. We also maintain a short-term memory within the agent that holds the context – for instance, if the agent logged into a site, we keep the session cookie in memory for subsequent steps in that session. If the agent opens multiple browser tabs or terminal processes, the orchestration keeps track of these contexts so it can switch as needed.

Security Considerations

Security is a critical aspect of the technical design, given the agent’s high level of autonomy:
- Sandboxed Execution: Every agent session runs in isolation. The Ubuntu VM is a sandbox that cannot directly affect other VMs or the host system. Even within the VM, we run the agent under a less-privileged Linux user account with only necessary permissions. If the agent tries to do something destructive (intentionally or by mistake), it is limited to its sandbox. For example, if it deletes files, it’s deleting within a disposable environment, not the user’s actual files. After each session, the VM can be reset to a clean snapshot to eliminate any persistent changes or malware that might have been inadvertently introduced.
- Command Safety Filters: We implement an internal filter on commands. Certain dangerous operations (like rm -rf / or formatting disk or making outbound network requests to unknown sites) can trigger a confirmation step or be blocked outright. The agent’s LLM will be instructed in its prompt to avoid obviously harmful instructions, but as a second layer, the system will catch any such command before execution. For web interactions, we maintain a list of disallowed domains (e.g. known malicious sites) and will prevent the agent from visiting them. If the agent needs to access a user’s personal accounts (like email or social media), we will use OAuth or token-based limited access rather than storing raw passwords, and sessions are isolated and wiped after use.
- User Approval & Oversight: In the user interface, we give an option for the user to run the agent in either fully-automatic mode or manual approval mode. In manual mode, before the agent executes each step (especially those that are sensitive, like sending an email or making a purchase), it will prompt the user describing the action (“I am about to execute: ‘Send $500 from your account to …’ – do you approve?”). This ensures that for critical actions, the user stays in control. For general usage with general users, our default will likely include some safe-check prompts until trust is established. Users can also interrupt or halt the agent at any time via a Stop button, which sends a termination signal to the VM.
- Data Privacy: All data the agent handles within a session (files processed, text of web pages, etc.) is kept confidential. If we log any data for debugging or model improvement, we will anonymize or get user consent. For enterprise users, we can offer an option where nothing is logged and the model prompts are encrypted at rest, to meet strict privacy requirements. Communications with model APIs are also secured; however, we acknowledge that using third-party AI APIs (OpenAI/Anthropic) means those providers will see the prompts and data. For highly sensitive tasks, we would have the option to route to an on-premise model or a local model running within the VM to avoid external data exposure.
- Scoping of Abilities: Initially, the agent’s abilities will be scoped to minimize risk. For example, it might be prevented from making financial transactions or modifying external systems unless explicitly enabled. As we gather more trust and refine safety, we can expand capabilities. This conservative rollout ensures we don’t end up with an agent that accidentally causes harm due to an overlooked scenario. We will also invite security audits and perhaps even a “red team” to test the agent’s resilience to prompt injection or malicious use.

In summary, by layering preventive measures (sandboxing, filters) and active monitoring (approvals, audits), we aim to provide a powerful agent that users (and enterprises) feel safe deploying on real tasks.

Scalability Considerations

Our architecture is built to scale horizontally to accommodate growing usage. Each user session (with its planning engine and VM) can run independently, so we scale by adding more servers/instances to handle more concurrent sessions:
- Stateless Orchestration: The central orchestrator that manages sessions is kept as stateless as possible. It can be replicated behind a load balancer – incoming user requests to start an agent can go to any orchestrator instance which then provisions a VM. Session state (like which VM is assigned to which user, the task logs, etc.) can be stored in a shared database or cache (like Redis). This way, even if one orchestrator instance fails or if we spin up multiple orchestrators globally, the system continues functioning. The design can automatically route a user to the nearest data center for lower latency, as all required data is in the shared database.
- Containerized VMs and Auto-Scaling: We will use container or VM orchestration (e.g. Kubernetes, Docker Swarm, or cloud provider’s native auto-scaling groups) to automatically adjust capacity. For instance, during peak demand, new VM containers are launched to serve more agents; during off-peak, extra ones are terminated to save cost. Each agent VM could also potentially host more than one agent task if resources permit (though isolation is simpler if one per VM). We’ll monitor CPU, memory, and API call load – if models become a bottleneck (e.g. hitting rate limits on the OpenAI API), we might queue tasks or incorporate additional model providers.
- Caching and Reuse: To improve scalability and efficiency, we might implement caching at multiple levels. For example, if many users ask the agent similar questions that require browsing the same website, the agent’s system could cache that webpage content so that subsequent agents can retrieve it without launching another browser instance. Similarly, results of common sub-tasks (like “download latest stock prices for X”) could be cached for a short time. This reduces duplicated work and speeds up responses, effectively increasing throughput.
- Model Optimizations: Since API calls to LLMs can be a scaling and cost bottleneck, we will optimize prompts to be as concise as possible (to reduce token usage) and use cheaper models when suitable. As our user base grows, we’ll consider fine-tuning smaller open-source models on our task data to eventually reduce reliance on expensive API calls. If a fine-tuned 13B parameter model (running on our own GPU server) can handle a subset of tasks with similar quality, we can route those tasks to it and reserve GPT-4/Claude for the most complex queries. This hybrid approach will help us maintain scalability cost-effectively.
- Load Testing and Redundancy: We will conduct extensive load testing to identify the breaking points of the system. Based on that, we add redundancy for critical components. For example, multiple instances of the browser automation service can run concurrently to handle many web navigation tasks at once. If one instance crashes (browsers can sometimes crash), the orchestrator can spin up a new one or reroute tasks. The database or state store will be a robust, scalable service (perhaps a managed cloud database with read replicas, or a distributed NoSQL store for session state). We also consider geographic scaling – deploying the service in multiple regions to serve global users with lower latency and to provide failover if one region has issues.
- Future Multi-Agent Scaling: In future improvements, we might enable one user’s task to be handled by multiple agent instances in parallel (for example, if a task can be split into subtasks). The architecture is amenable to this: a high-level manager could coordinate multiple agent workers. We mention this to highlight that our design is not just about scaling users, but also scaling complexity – tackling bigger problems by allocating more agents. This could be a differentiator for enterprise use (solving large-scale data problems by distributing work across agents).

Overall, by using cloud-native scaling techniques and efficient resource management, the platform will smoothly handle an increasing number of users and tasks. Scalability is built in from day one by the decision to use cloud VMs and stateless services, ensuring that as demand grows, the user experience remains fast and reliable.
